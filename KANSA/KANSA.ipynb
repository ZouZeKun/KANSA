{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, argparse, json, time\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from texttable import Texttable\n",
    "from typing import Union, Tuple, Dict, List\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()  \n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=10, help='Random seed of the experiment')\n",
    "    parser.add_argument('--exp_name', type=str, default='Exp', help='Name of the experiment')\n",
    "    \n",
    "    parser.add_argument('--num_step', type=int, default=8640, help='4*24*90=8640') # TODO 90days\n",
    "    parser.add_argument('--batch_size', type=int, default=96, help='Size of the batch') \n",
    "    parser.add_argument('--train_ratio' , type=float, default=3/6, help='train set ratio') \n",
    "    parser.add_argument('--val_ratio'   , type=float, default=1/6, help='validation set ratio')\n",
    "    parser.add_argument('--test_ratio'  , type=float, default=2/6, help='test set ratio') \n",
    "\n",
    "    parser.add_argument('--num_history', type=int, default=1, help='Number of historical time steps') \n",
    "    parser.add_argument('--ChebyshevDegree', type=int, default=8, help='degree of KAN') \n",
    "    parser.add_argument('--FourierDegree', type=int, default=6, help='degree of KAN') \n",
    "\n",
    "    parser.add_argument('--num_nodes', type=int, default=24, help='Number of nodes in the graph')\n",
    "    parser.add_argument('--num_edges', type=int, default=34, help='Number of edges in the graph') \n",
    "    parser.add_argument('--hidden_dim', type=int, default=12, help='Dimension of the hidden layers') \n",
    "    \n",
    "    parser.add_argument('--max_epoch', type=int, default=50, help='Maximum number of epochs') \n",
    "    parser.add_argument('--learning_rate', type=float, default=2e-5, help='Learning rate of Adam') \n",
    "    \n",
    "    # L1-norm\n",
    "    parser.add_argument('--l1_lambda', type=float, default=0.00, help='L1 regularization coefficient')\n",
    "    # L2-norm\n",
    "    parser.add_argument('--l2_lambda', type=float, default=0.00, help='L2 regularization coefficient')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=0.95, help='decay parameter') \n",
    "    parser.add_argument('--decay_epoch', type=float, default=1, help='decay epoch') \n",
    "    parser.add_argument('--patience', type=int, default=10, help='Patience parameter for early stopping') \n",
    "\n",
    "    parser.add_argument('--pressure_sensor_name', type=str, default='[\"4\", \"13\", \"14\", \"23\", \"24\"]', help='pressure sensor index')\n",
    "    parser.add_argument('--flow_sensor_name', type=str, default='[\"4\", \"7\", \"25\", \"28\", \"34\"]', help='flow sensor index')\n",
    "    parser.add_argument('--demand_sensor_name', type=str, default='[\"2\", \"16\", \"19\", \"22\", \"24\"]', help='demand sensor index')\n",
    "    \n",
    "    parser.add_argument('--water_pressure_file', type=str, default=\"./dataset/simulate_pressure3.csv\")\n",
    "    parser.add_argument('--water_flow_file', type=str, default=\"./dataset/simulate_flow3.csv\")\n",
    "    parser.add_argument('--water_demand_file', type=str, default=\"./dataset/simulate_demand3.csv\")\n",
    "\t\n",
    "    parser.add_argument('--node_file', type=str, default='./dataset/node_features_Apulia.csv') # (Reserved Interface)\n",
    "    parser.add_argument('--edge_file', type=str, default='./dataset/edge_index_dir.csv') \t   # (Reserved Interface)\n",
    "\n",
    "    parser.add_argument('--incidence_file', type=str, default='./dataset/initial_incidence_matrix_dir.csv')\n",
    "    parser.add_argument('--cycle_file', type=str, default='./dataset/initial_cycle_matrix_dir.csv')\n",
    "\n",
    "    args = parser.parse_args(args=[])  \n",
    "\n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数 MSE\n",
    "class ConservationConstraints(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConservationConstraints, self).__init__()\n",
    "\n",
    "    def forward(self, pressure_Y, flow_Y, demand_Y, PipeFriction, IncidenceMatrix, CycleMatrix):\n",
    "        \n",
    "        # Conservation Constraints —— (10.67 Q^1.852L)/(C^1.852 D^4.87)\n",
    "        PipeFriction = PipeFriction.view(1, 1, -1) \n",
    "        pipe_pressure_minus = (abs(flow_Y)**1.852) * PipeFriction \n",
    "        pipe_pressure_minus = torch.where(flow_Y > 0, \n",
    "                                        pipe_pressure_minus, \n",
    "                                        -pipe_pressure_minus)\n",
    "\n",
    "        loss2_1 = torch.sum((torch.abs((flow_Y @ IncidenceMatrix.t()) + demand_Y)*100)** 2)/96           #  100 -->  targer<0.01  (Nodal flow equation)\n",
    "        loss2_2 = torch.sum((torch.abs((pipe_pressure_minus @ CycleMatrix.t()))*10)** 2)/96              #  10  -->  targer<0.1   (loop energy equation)\n",
    "        loss2_3 = torch.sum((torch.abs((pressure_Y @ IncidenceMatrix) - pipe_pressure_minus)*10)** 2)/96 #  10  -->  targer<0.1   (Pipe pressure drop equation)\n",
    "\n",
    "        return loss2_1, loss2_2, loss2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConservationConstraints_generalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConservationConstraints_generalization, self).__init__()\n",
    "        \n",
    "    def forward(self, pressure_Y, flow_Y, demand_Y, PipeFriction, IncidenceMatrix, CycleMatrix):\n",
    "        \n",
    "        # Conservation Constraints —— (10.67 Q^1.852L)/(C^1.852 D^4.87)\n",
    "        PipeFriction = PipeFriction.view(1,  -1) \n",
    "        pipe_pressure_minus = (abs(flow_Y)**1.852) * PipeFriction \n",
    "        pipe_pressure_minus = torch.where(flow_Y > 0, \n",
    "                                        pipe_pressure_minus, \n",
    "                                        -pipe_pressure_minus)\n",
    "        loss2_1 = torch.sum((torch.abs((flow_Y @ IncidenceMatrix.t()) + demand_Y))** 2, dim=0)/96            #  100 -->  targer<0.01 (Nodal flow equation)\n",
    "        loss2_2 = torch.sum((torch.abs((pipe_pressure_minus @ CycleMatrix.t())))** 2, dim=0)/96              #  10  -->  targer<0.1  (loop energy equation)\n",
    "        loss2_3 = torch.sum((torch.abs((pressure_Y @ IncidenceMatrix) - pipe_pressure_minus))** 2, dim=0)/96 #  10  -->  targer<0.1  (Pipe pressure drop equation)\n",
    "        \n",
    "        return loss2_1, loss2_2, loss2_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(args, pressure_data, flow_data, demand_data):\n",
    "    ''' \n",
    "    生成两步掩码：一步掩码张量用于掩盖只剩传感器，二步掩码张量用于继续掩盖传感器\n",
    "    1. 只有传感器：masked_tensor1 / 掩码方式：mask1\n",
    "    2. 再对传感器进行掩盖：masked_tensor2 / 掩码方式：mask2\n",
    "    '''\n",
    "    pressure_sensor_name = json.loads(args.pressure_sensor_name)\n",
    "    flow_sensor_name = json.loads(args.flow_sensor_name)\n",
    "    demand_sensor_name = json.loads(args.demand_sensor_name)\n",
    "\n",
    "    pressure_sensor_index = [int(i)-1 for i in pressure_sensor_name]\n",
    "    flow_sensor_index = [int(i)-1 for i in flow_sensor_name]\n",
    "    demand_sensor_index = [int(i)-1 for i in demand_sensor_name]\n",
    "\n",
    "    mask1_pressure = torch.zeros(pressure_data.shape)\n",
    "    mask1_flow     = torch.zeros(flow_data.shape)\n",
    "    mask1_demand   = torch.zeros(pressure_data.shape)\n",
    "    mask1_pressure[:,pressure_sensor_index] = 1\n",
    "    mask1_flow[:,flow_sensor_index] = 1\n",
    "    mask1_demand[:,demand_sensor_index] = 1\n",
    "    mask1_demand_reduced = mask1_demand[:, :-1]\n",
    "\n",
    "    mask1_pressure = mask1_pressure.to(args.device)\n",
    "    mask1_flow = mask1_flow.to(args.device)\n",
    "    mask1_demand_reduced = mask1_demand_reduced.to(args.device)\n",
    "    \n",
    "    return (mask1_pressure, mask1_flow, mask1_demand_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2instance(args, data):\n",
    "    num_step, dims = data.shape\n",
    "    # Each additional step generates a training sample.\n",
    "    num_sample = num_step - args.batch_size - args.batch_size + 1\n",
    "\n",
    "    x = torch.zeros(num_sample, args.batch_size, dims)\n",
    "    y = torch.zeros(num_sample, args.batch_size, dims)\n",
    "    \n",
    "    for i in range(num_sample):\n",
    "        x[i] = data[i: i + args.batch_size]\n",
    "        y[i] = data[i: i + args.batch_size]\n",
    "    return x, y\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, args, dataset_type='train'):\n",
    "        self.args = args\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "        train_steps = round(args.train_ratio * args.num_step)\n",
    "        test_steps = round(args.test_ratio * args.num_step)\n",
    "        val_steps = args.num_step - train_steps - test_steps\n",
    "\n",
    "        NodePressure = pd.read_csv(args.water_pressure_file, header=0, index_col=0)\n",
    "        PipeFlow = pd.read_csv(args.water_flow_file, header=0, index_col=0)\n",
    "        NodeDemand = pd.read_csv(args.water_demand_file, header=0, index_col=0)\n",
    "\n",
    "        # global min-max \n",
    "        NodePressure_ = (NodePressure - NodePressure.min().min()) / (NodePressure.max().max() - NodePressure.min().min())\n",
    "        PipeFlow_ = (PipeFlow - PipeFlow.min().min()) / (PipeFlow.max().max() - PipeFlow.min().min())\n",
    "        NodeDemand_ = (NodeDemand - (NodeDemand.iloc[:, :-1]).min().min()) / (NodeDemand.max().max() - (NodeDemand.iloc[:, :-1]).min().min())\n",
    "\n",
    "        NodePressure = torch.FloatTensor(NodePressure_.values)\n",
    "        PipeFlow = torch.FloatTensor(PipeFlow_.values)\n",
    "        NodeDemand = torch.FloatTensor(NodeDemand_.values)\n",
    "\n",
    "        # split dataset\n",
    "        self.train_pressure = NodePressure[: train_steps]\n",
    "        self.train_flow     = PipeFlow[: train_steps]\n",
    "        self.train_demand   = NodeDemand[: train_steps]\n",
    "\n",
    "        self.val_pressure   = NodePressure[train_steps: train_steps + val_steps]\n",
    "        self.val_flow       = PipeFlow[train_steps: train_steps + val_steps]\n",
    "        self.val_demand     = NodeDemand[train_steps: train_steps + val_steps]\n",
    "\n",
    "        self.test_pressure  = NodePressure[-test_steps:]\n",
    "        self.test_flow      = PipeFlow[-test_steps:]\n",
    "        self.test_demand    = NodeDemand[-test_steps:]\n",
    "\n",
    "        self.trainX_pressure, self.trainY_pressure = seq2instance(args, self.train_pressure)\n",
    "        self.trainX_flow, self.trainY_flow = seq2instance(args, self.train_flow)\n",
    "        self.trainX_demand, self.trainY_demand = seq2instance(args, self.train_demand)\n",
    "\n",
    "        self.valX_pressure, self.valY_pressure = seq2instance(args, self.val_pressure)\n",
    "        self.valX_flow, self.valY_flow = seq2instance(args, self.val_flow)\n",
    "        self.valX_demand, self.valY_demand = seq2instance(args, self.val_demand)\n",
    "\n",
    "        self.testX_pressure, self.testY_pressure = seq2instance(args, self.test_pressure)\n",
    "        self.testX_flow, self.testY_flow = seq2instance(args, self.test_flow)\n",
    "        self.testX_demand, self.testY_demand = seq2instance(args, self.test_demand)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataset_type == 'train':\n",
    "            return len(self.trainX_pressure)\n",
    "        elif self.dataset_type == 'val':\n",
    "            return len(self.valX_pressure)\n",
    "        elif self.dataset_type == 'test':\n",
    "            return len(self.testX_pressure)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.dataset_type == 'train':\n",
    "            return (self.trainX_pressure[idx].to(self.args.device), self.trainY_pressure[idx].to(self.args.device),\n",
    "                    self.trainX_flow[idx].to(self.args.device), self.trainY_flow[idx].to(self.args.device),\n",
    "                    self.trainX_demand[idx].to(self.args.device), self.trainY_demand[idx].to(self.args.device)\n",
    "                    )\n",
    "        \n",
    "        elif self.dataset_type == 'val':\n",
    "            return (self.valX_pressure[idx].to(self.args.device), self.valY_pressure[idx].to(self.args.device),\n",
    "                    self.valX_flow[idx].to(self.args.device), self.valY_flow[idx].to(self.args.device),\n",
    "                    self.valX_demand[idx].to(self.args.device), self.valY_demand[idx].to(self.args.device)\n",
    "                    )\n",
    "\n",
    "        elif self.dataset_type == 'test':\n",
    "            return (self.testX_pressure[idx].to(self.args.device), self.testY_pressure[idx].to(self.args.device), \n",
    "                    self.testX_flow[idx].to(self.args.device), self.testY_flow[idx].to(self.args.device),\n",
    "                    self.testX_demand[idx].to(self.args.device), self.testY_demand[idx].to(self.args.device)\n",
    "                    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix(args):\n",
    "    # node file\n",
    "    nodes_df = pd.read_csv(args.node_file, header=0, index_col=0)\n",
    "    nodes_attr = torch.from_numpy(nodes_df.values).float().to(args.device)\n",
    "\n",
    "    # edge file —— edge weight (Hazen-Williams)\n",
    "    edge_df = pd.read_csv(args.edge_file, header=0, index_col=0)\n",
    "    edge_index  = edge_df.iloc[:, 0:2].T  \n",
    "    edge_attr = edge_df.iloc[:, 2:7]     \n",
    "    edge_index = torch.from_numpy(edge_index.values).long().to(args.device)\n",
    "    edge_attr = torch.from_numpy(edge_attr.values).float().to(args.device)\n",
    "\n",
    "    Graph_Data = Data(x=nodes_attr, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    PipeFriction = edge_df.iloc[:, -1].values\n",
    "    pipe_friction = torch.from_numpy(PipeFriction).float().to(args.device)\n",
    "    \n",
    "    # incidence matrix\n",
    "    incidence_matrix_df = pd.read_csv(args.incidence_file, header=None, index_col=None)\n",
    "    incidence_matrix = torch.from_numpy(incidence_matrix_df.values).float().to(args.device)\n",
    "    # cycle matrix\n",
    "    cycle_matrix_df = pd.read_csv(args.cycle_file, header=0, index_col=0)\n",
    "    cycle_matrix = torch.from_numpy(cycle_matrix_df.values).float().to(args.device)\n",
    "\n",
    "    return Graph_Data, pipe_friction, incidence_matrix, cycle_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IOStream():\n",
    "    \"\"\"训练日志文件\"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.file = open(path, 'a') \n",
    "\n",
    "    def cprint(self, text):\n",
    "        print(text)\n",
    "        self.file.write(text + '\\n')\n",
    "        self.file.flush() \n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "def table_printer(args):\n",
    "    \"\"\"绘制参数表格\"\"\"\n",
    "    args = vars(args) \n",
    "    keys = sorted(args.keys()) \n",
    "    table = Texttable()\n",
    "    table.set_cols_dtype(['t', 't']) \n",
    "    rows = [[\"Parameter\", \"Value\"]] \n",
    "    for k in keys:\n",
    "        rows.append([k.replace(\"_\", \" \").capitalize(), str(args[k])]) \n",
    "    table.add_rows(rows)\n",
    "    return table.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAN Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于切比雪夫基函数的KAN层\n",
    "class ChebyshevKANLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, degree):\n",
    "        super(ChebyshevKANLayer, self).__init__()\n",
    "        self.inputdim = input_dim\n",
    "        self.outdim = output_dim\n",
    "        self.ChebyshevDegree = degree\n",
    "        self.addbias  = True\n",
    "        self.cheby_coeffs = nn.Parameter(torch.empty(input_dim, output_dim, degree + 1))\n",
    "        nn.init.normal(self.cheby_coeffs, mean=0.0, std=1/(input_dim * (degree + 1)))\n",
    "        self.bias = nn.Parameter(torch.zeros(1, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xshp = x.shape                          # (seq_length, inputdim)\n",
    "        outshape = xshp[0:-1] + (self.outdim,)  # (seq_length, outdim)\n",
    "        x = x.view(-1, self.inputdim)           # (seq_length, inputdim)\n",
    "\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        cheby = torch.ones(x.shape[0], self.inputdim, self.ChebyshevDegree + 1, device=x.device)\n",
    "        if self.ChebyshevDegree > 0:\n",
    "            cheby[:, :, 1] = x\n",
    "        for i in range(2, self.ChebyshevDegree + 1):\n",
    "            cheby[:, :, i] = 2 * x * cheby[:, :, i - 1].clone() - cheby[:, :, i - 2].clone()\n",
    "\n",
    "        # Compute the Chebyshev interpolation\n",
    "        # (seq_length, inputdim, degree+1) x (inputdim, output_dim, degree+1) -> (seq_length, output_dim)\n",
    "        y = torch.einsum('bid,iod->bo', cheby, self.cheby_coeffs)\n",
    "\n",
    "        # y += self.bias\n",
    "\n",
    "        y = y.view(outshape) \n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 傅里叶基函数\n",
    "class FourierKANLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, degree):\n",
    "        super(FourierKANLayer,self).__init__()\n",
    "        self.gridsize = degree\n",
    "        self.addbias  = True\n",
    "        self.inputdim = input_dim\n",
    "        self.outdim = output_dim\n",
    "\n",
    "        self.fouriercoeffs = nn.Parameter(\n",
    "                    torch.randn(2, output_dim, input_dim, degree) / \n",
    "                    (np.sqrt(input_dim) * np.sqrt(self.gridsize))\n",
    "                     )\n",
    "        nn.init.normal(self.fouriercoeffs, mean=0.0, std=1 / (np.sqrt(input_dim) * np.sqrt(self.gridsize)))\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(1, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xshp = x.shape                          # (batch_size, seq_length, inputdim)\n",
    "        outshape = xshp[0:-1] + (self.outdim,)  # (batch_size, seq_lengtha, outdim)\n",
    "        x = torch.tanh(x)     \n",
    "\n",
    "        x = x.view(-1, self.inputdim)           # (batch_size*seq_length, inputdim)\n",
    "\n",
    "        # Fourier basis function -- (1, 1, 1, gridsize)\n",
    "        k = torch.reshape(torch.arange(1, self.gridsize+1, device=x.device), (1, 1, 1, self.gridsize))\n",
    "        xrshp = x.view(x.shape[0], 1, x.shape[1], 1)   # (batch_size*seq_length, 1, inputdim, 1)\n",
    "\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "\n",
    "        y1 = torch.sum(c * self.fouriercoeffs[0:1], (-2, -1)) \n",
    "        y2 = torch.sum(s * self.fouriercoeffs[1:2], (-2, -1))\n",
    "\n",
    "        y = y1 + y2\n",
    "        \n",
    "        # y += self.bias\n",
    "\n",
    "        # (batch_size, seq_length, outdim)\n",
    "        y = y.view(outshape) \n",
    "        \n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力机制\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_layer   = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_layer = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.layer_norm  = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, attention_from, attention_to):\n",
    "        # 计算注意力分数\n",
    "        query = self.query_layer(attention_from)    # (batch_size, hidden_size)\n",
    "        key = self.key_layer(attention_to)          # (batch_size, hidden_size)\n",
    "        value = self.value_layer(attention_from)    # (batch_size, hidden_size)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        scores = torch.matmul(query, key.T)                # (batch_size, batch_size)\n",
    "        attention_weights = F.softmax(scores, dim=-1)     \n",
    "\n",
    "        # 计算加权输出\n",
    "        output = torch.matmul(attention_weights, value)  # (batch_size, hidden_size)\n",
    "        output = self.layer_norm(output)  # (batch_size, hidden_size)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANSA(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param num_layers: number of KANSA layers\n",
    "\n",
    "        :param input_node_dim: input dimension of node features\n",
    "        :param input_edge_dim: input dimension of edge features\n",
    "\n",
    "        :param max_path_distance: max pairwise distance between two nodes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.batch_size = args.batch_size\n",
    "        self.ChebyshevDegree = args.ChebyshevDegree\n",
    "        self.FourierDegree = args.FourierDegree\n",
    "\n",
    "        self.num_nodes = args.num_nodes\n",
    "        self.num_edges = args.num_edges\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "\n",
    "        ''' input ''' \n",
    "        # Temporal features\n",
    "        self.flow_batch = nn.Sequential(\n",
    "            nn.Linear(self.batch_size, self.hidden_dim),  \n",
    "            nn.Linear(self.hidden_dim, self.batch_size),\n",
    "            FourierKANLayer(self.batch_size, self.hidden_dim, self.FourierDegree),  \n",
    "            FourierKANLayer(self.hidden_dim, self.batch_size, self.FourierDegree),   \n",
    "            nn.LayerNorm(self.batch_size),            \n",
    "            )\n",
    "        self.pressure_batch = nn.Sequential(\n",
    "            nn.Linear(self.batch_size, self.hidden_dim), \n",
    "            nn.Linear(self.hidden_dim, self.batch_size),\n",
    "            FourierKANLayer(self.batch_size, self.hidden_dim, self.FourierDegree), \n",
    "            FourierKANLayer(self.hidden_dim, self.batch_size, self.FourierDegree),   \n",
    "            nn.LayerNorm(self.batch_size),    \n",
    "            )\n",
    "        self.demand_batch = nn.Sequential(\n",
    "            nn.Linear(self.batch_size, self.hidden_dim), \n",
    "            nn.Linear(self.hidden_dim, self.batch_size),\n",
    "            FourierKANLayer(self.batch_size, self.hidden_dim, self.FourierDegree),   \n",
    "            FourierKANLayer(self.hidden_dim, self.batch_size, self.FourierDegree),   \n",
    "            nn.LayerNorm(self.batch_size),      \n",
    "            )\n",
    "        # Spatial features\n",
    "        self.flow_features = nn.Sequential(\n",
    "            nn.Linear(self.num_edges, self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.num_edges), \n",
    "            ChebyshevKANLayer(self.num_edges, self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(self.hidden_dim, self.num_edges, self.ChebyshevDegree), \n",
    "            nn.LayerNorm(self.num_edges),\n",
    "            )\n",
    "        self.pressure_features = nn.Sequential(\n",
    "            nn.Linear(self.num_nodes, self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.num_nodes),\n",
    "            ChebyshevKANLayer(self.num_nodes, self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(self.hidden_dim, self.num_nodes, self.ChebyshevDegree), \n",
    "            nn.LayerNorm(self.num_nodes), \n",
    "            )\n",
    "        self.demand_features = nn.Sequential(\n",
    "            nn.Linear(self.num_nodes-1, self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.num_nodes-1),\n",
    "            ChebyshevKANLayer(self.num_nodes-1, self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(self.hidden_dim, self.num_nodes-1, self.ChebyshevDegree), \n",
    "            nn.LayerNorm(self.num_nodes-1), \n",
    "            )\n",
    "        # Self-attention temporal\n",
    "        self.attention_pressure_batch = Attention(self.batch_size)\n",
    "        self.attention_flow_batch     = Attention(self.batch_size)\n",
    "        self.attention_demand_batch   = Attention(self.batch_size)\n",
    "        # Self-attention spatial\n",
    "        self.attention_pressure_features = Attention(self.num_nodes)\n",
    "        self.attention_flow_features     = Attention(self.num_edges)\n",
    "        self.attention_demand_features   = Attention(self.num_nodes-1)\n",
    "\n",
    "        self.pressure_1 = nn.Sequential(\n",
    "            nn.LayerNorm(self.num_nodes),\n",
    "            ChebyshevKANLayer(self.num_nodes, self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(self.hidden_dim, self.hidden_dim, self.ChebyshevDegree)\n",
    "            )\n",
    "        self.pressure_2 = nn.Sequential(\n",
    "            nn.LayerNorm(self.num_nodes),\n",
    "            ChebyshevKANLayer(self.num_nodes, self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(self.hidden_dim, self.hidden_dim, self.ChebyshevDegree),\n",
    "            )\n",
    "        self.flow_1 = nn.Sequential(\n",
    "            nn.LayerNorm(self.num_edges),\n",
    "            ChebyshevKANLayer(self.num_edges, self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(self.hidden_dim, self.hidden_dim, self.ChebyshevDegree),\n",
    "            )\n",
    "        self.flow_2 = nn.Sequential(\n",
    "            nn.LayerNorm(self.num_edges),\n",
    "            ChebyshevKANLayer(self.num_edges, self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(self.hidden_dim, self.hidden_dim, self.ChebyshevDegree),\n",
    "            )\n",
    "        self.demand_1 = nn.Sequential(\n",
    "            nn.LayerNorm(self.num_nodes-1),\n",
    "            ChebyshevKANLayer(self.num_nodes-1, self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(self.hidden_dim, self.hidden_dim, self.ChebyshevDegree),\n",
    "            )\n",
    "        self.demand_2 = nn.Sequential(\n",
    "            nn.LayerNorm(self.num_nodes-1),\n",
    "            ChebyshevKANLayer(self.num_nodes-1, self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(self.hidden_dim, self.hidden_dim, self.ChebyshevDegree),\n",
    "            )\n",
    "\n",
    "        self.pressure_sum = nn.Sequential(\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "            ChebyshevKANLayer(self.hidden_dim, 2*self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(2*self.hidden_dim, 2*self.hidden_dim, self.ChebyshevDegree),\n",
    "            )\n",
    "        self.flow_sum = nn.Sequential(\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "            ChebyshevKANLayer(self.hidden_dim, 2*self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(2*self.hidden_dim, 2*self.hidden_dim, self.ChebyshevDegree),\n",
    "            )\n",
    "        self.demand_sum = nn.Sequential(\n",
    "            nn.LayerNorm(self.hidden_dim),\n",
    "            ChebyshevKANLayer(self.hidden_dim, 2*self.hidden_dim, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(2*self.hidden_dim, 2*self.hidden_dim, self.ChebyshevDegree),\n",
    "            )\n",
    "\n",
    "        # Processor\n",
    "        self.graph = nn.Sequential(\n",
    "            nn.LayerNorm(3),\n",
    "            ChebyshevKANLayer(3, 12, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(12, 1, self.ChebyshevDegree),\n",
    "            )\n",
    "\n",
    "        # Decoding\n",
    "        self.flow_out = nn.Sequential(\n",
    "            ChebyshevKANLayer(2*self.hidden_dim, 2*self.num_edges, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(2*self.num_edges, self.num_edges, self.ChebyshevDegree),\n",
    "            nn.Linear(self.num_edges, self.num_edges),\n",
    "            nn.Linear(self.num_edges, self.num_edges),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "        self.pressure_out = nn.Sequential(\n",
    "            ChebyshevKANLayer(2*self.hidden_dim, 2*self.num_nodes, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(2*self.num_nodes, self.num_nodes, self.ChebyshevDegree),\n",
    "            nn.Linear(self.num_nodes, self.num_nodes),\n",
    "            nn.Linear(self.num_nodes, self.num_nodes),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "        # The last column of Demand is negative (only the previous columns are estimated)\n",
    "        self.demand_out = nn.Sequential(\n",
    "            ChebyshevKANLayer(2*self.hidden_dim, 2*self.num_nodes, self.ChebyshevDegree),\n",
    "            ChebyshevKANLayer(2*self.num_nodes, self.num_nodes-1, self.ChebyshevDegree),\n",
    "            nn.Linear(self.num_nodes-1, self.num_nodes-1),\n",
    "            nn.Linear(self.num_nodes-1, self.num_nodes-1),\n",
    "            nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "    def forward(self, Graph_Data, \n",
    "                mask_pressure, mask_flow, mask_demand,\n",
    "                pressure, flow, demand) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        pressure_ = pressure.float()     # (batch_size, num_nodes)\n",
    "        flow_ = flow.float()             # (batch_size, num_edges)\n",
    "        demand_ = demand.float()         # (batch_size, num_nodes)\n",
    "\n",
    "        # Temporal\n",
    "        pressure_batch      = self.pressure_batch(pressure_.T)     # (batch_size, num_nodes)\n",
    "        flow_batch          = self.flow_batch(flow_.T)             # (batch_size, num_edges)\n",
    "        demand_batch        = self.demand_batch(demand_.T)         # (batch_size, num_nodes)\n",
    "        pressure_batch      = (self.attention_pressure_batch(pressure_batch, pressure_.T)).T       # (batch_size, num_nodes)\n",
    "        flow_batch          = (self.attention_flow_batch(flow_batch, flow_.T)).T                   # (batch_size, num_edges)\n",
    "        demand_batch        = (self.attention_demand_batch(demand_batch, demand_.T)).T             # (batch_size, num_nodes)\n",
    "        # Spatial\n",
    "        pressure_features   = self.pressure_features(pressure_)    # (batch_size, num_nodes)\n",
    "        flow_features       = self.flow_features(flow_)            # (batch_size, num_edges)\n",
    "        demand_features     = self.demand_features(demand_)        # (batch_size, num_nodes)\n",
    "        pressure_features   = self.attention_pressure_features(pressure_features, pressure_)       # (batch_size, num_nodes)\n",
    "        flow_features       = self.attention_flow_features(flow_features, flow_)                   # (batch_size, num_edges)\n",
    "        demand_features     = self.attention_demand_features(demand_features, demand_)             # (batch_size, num_nodes)\n",
    "\n",
    "        #  \"*\" is better\n",
    "        pressure_batch_    = self.pressure_1(pressure_batch * (1-mask_pressure) + pressure_)\n",
    "        pressure_features_ = self.pressure_2(pressure_features * (1-mask_pressure) + pressure_)\n",
    "        pressure_st_       = pressure_batch_ * pressure_features_\n",
    "        flow_batch_        = self.flow_1(flow_batch * (1-mask_flow) + flow_)\n",
    "        flow_features_     = self.flow_2(flow_features * (1-mask_flow) + flow_)\n",
    "        flow_st_           = flow_batch_ * flow_features_  \n",
    "        demand_batch_      = self.demand_1(demand_batch * (1-mask_demand)  + demand_)\n",
    "        demand_features_   = self.demand_2(demand_features * (1-mask_demand) + demand_)\n",
    "        demand_st_         = demand_batch_ * demand_features_\n",
    "\n",
    "        # Concat --> Processor\n",
    "        Graph_features = torch.cat((self.pressure_sum(pressure_st_).unsqueeze(-1), \n",
    "                                    self.flow_sum(flow_st_).unsqueeze(-1), \n",
    "                                    self.demand_sum(demand_st_).unsqueeze(-1)), \n",
    "                                    dim=-1)\n",
    "        graph_features = self.graph(Graph_features).squeeze(-1)\n",
    "\n",
    "        # Decoding\n",
    "        # Residual connection directly replaces the output\n",
    "        pressure_dd = self.pressure_out(graph_features) * (1-mask_pressure)   + pressure_ \n",
    "        flow_dd = self.flow_out(graph_features)         * (1-mask_flow)       + flow_ \n",
    "        demand_dd = self.demand_out(graph_features)     * (1-mask_demand)     + demand_\n",
    "\n",
    "        return pressure_dd, flow_dd, demand_dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, IO,  train_loader, val_loader, \n",
    "        min_pressure, max_pressure, min_flow, max_flow, min_demand, max_demand):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = args.patience  \n",
    "    patience_counter = 0\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    torch.cuda.manual_seed(args.seed)  \n",
    "    \n",
    "    Graph_Data, pipe_friction, incidence_matrix, cycle_matrix = load_matrix(args)\n",
    "\n",
    "    model = KANSA(args).to(device)\n",
    "\n",
    "    IO.cprint(str(model))\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    IO.cprint('Model Parameter: {}'.format(total_params))\n",
    "    \n",
    "    # RMSprop  (Trick)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                        step_size=args.decay_epoch,\n",
    "                                        gamma=args.gamma)\n",
    "    IO.cprint('Using RMSprop')\n",
    "\n",
    "    # Loss Function\n",
    "    criterion = ConservationConstraints()\n",
    "    # Save\n",
    "    train_loss1_list = []\n",
    "    train_loss2_list = []\n",
    "    train_loss3_list = []\n",
    "    val_loss1_list = []\n",
    "    val_loss2_list = []\n",
    "    val_loss3_list = []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args.max_epoch):\n",
    "        #################\n",
    "        ###   Train   ###\n",
    "        #################\n",
    "        model.train()  \n",
    "        train_loss1 = 0.0 \n",
    "        train_loss2 = 0.0\n",
    "        train_loss3 = 0.0\n",
    "        pressure_True = torch.Tensor().to(args.device)\n",
    "        pressure_Est  = torch.Tensor().to(args.device)\n",
    "        flow_True     = torch.Tensor().to(args.device)\n",
    "        flow_Est      = torch.Tensor().to(args.device)\n",
    "        demand_True   = torch.Tensor().to(args.device)\n",
    "        demand_Est    = torch.Tensor().to(args.device)\n",
    "\n",
    "        for i, data in tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Train_Loader\"):\n",
    "            # 加载数据\n",
    "            (train_pressureX, train_pressureY, \n",
    "            train_flowX, train_flowY, \n",
    "            train_demandX, train_demandY) = data\n",
    "            train_pressureX = train_pressureX.squeeze(0)\n",
    "            train_flowX     = train_flowX.squeeze(0)\n",
    "            train_demandX   = train_demandX.squeeze(0)\n",
    "\n",
    "            # Random Masking\n",
    "            (mask1_pressure, mask1_flow, mask1_demand)=apply_mask(args, train_pressureX, train_flowX, train_demandX)\n",
    "\n",
    "            # masking\n",
    "            train_pressureX_ = train_pressureX * mask1_pressure\n",
    "            train_flowX_     = train_flowX * mask1_flow\n",
    "            train_demandX_   = (train_demandX[:, :-1]) * mask1_demand\n",
    "\n",
    "            # model\n",
    "            pressure_Y, flow_Y, demand_Y = model(Graph_Data, mask1_pressure, mask1_flow, mask1_demand, \n",
    "                                                train_pressureX_, train_flowX_, train_demandX_)\n",
    "            # Add last column to demand_Y\n",
    "            row_sums = torch.sum(demand_Y, dim=1, keepdim=True)\n",
    "            demand_Y = torch.cat((demand_Y, -row_sums), dim=1)\n",
    "\n",
    "            # min-max normalized reduction\n",
    "            pressure_Y = (pressure_Y * (max_pressure - min_pressure) + min_pressure)\n",
    "            flow_Y     = (flow_Y     * (max_flow     - min_flow)     + min_flow)\n",
    "            demand_Y   = (demand_Y   * (max_demand   - min_demand)   + min_demand)\n",
    "            train_pressureX  = (train_pressureX  * (max_pressure - min_pressure) + min_pressure)\n",
    "            train_pressureX_ = (train_pressureX_ * (max_pressure - min_pressure) + min_pressure)\n",
    "            train_flowX      = (train_flowX      * (max_flow     - min_flow)     + min_flow)\n",
    "            train_flowX_     = (train_flowX_     * (max_flow     - min_flow)     + min_flow)\n",
    "            train_demandX    = (train_demandX    * (max_demand   - min_demand)   + min_demand)\n",
    "            train_demandX_   = (train_demandX_   * (max_demand   - min_demand)   + min_demand)\n",
    "            \n",
    "            # Caculate Loss\n",
    "            (loss1, loss2, loss3) = criterion(pressure_Y, flow_Y, demand_Y, pipe_friction, incidence_matrix, cycle_matrix)\n",
    "\n",
    "            # Save Data\n",
    "            pressure_True = torch.cat((pressure_True, train_pressureX), 0)\n",
    "            pressure_Est  = torch.cat((pressure_Est, pressure_Y), 0)\n",
    "            flow_True     = torch.cat((flow_True,   train_flowX), 0)\n",
    "            flow_Est      = torch.cat((flow_Est,     flow_Y), 0)\n",
    "            demand_True   = torch.cat((demand_True, train_demandX), 0)\n",
    "            demand_Est    = torch.cat((demand_Est,   demand_Y), 0)\n",
    "\n",
    "            # l1+l2 norm \n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            loss_ = loss1 + loss2 + loss3\n",
    "\n",
    "            loss = loss_ + args.l1_lambda * l1_norm + args.l2_lambda * l2_norm\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss1 += (loss1).item()\n",
    "            train_loss2 += (loss2).item()\n",
    "            train_loss3 += (loss3).item()\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_train_loss1 = (train_loss1) / len(train_loader.dataset)\n",
    "        avg_train_loss2 = (train_loss2) / len(train_loader.dataset)\n",
    "        avg_train_loss3 = (train_loss3) / len(train_loader.dataset)\n",
    "\n",
    "        train_loss1_list.append(avg_train_loss1)\n",
    "        train_loss2_list.append(avg_train_loss2)\n",
    "        train_loss3_list.append(avg_train_loss3)\n",
    "\n",
    "        IO.cprint('Epoch #{:03d}, Conservation1: {:.4f}, Conservation2: {:.4f}, Conservation3: {:.4f}'.format(\n",
    "                    epoch, (avg_train_loss1), (avg_train_loss2), (avg_train_loss3)\n",
    "                    ))\n",
    "\n",
    "        #################\n",
    "        ###   Valid   ###\n",
    "        #################\n",
    "        model.eval()\n",
    "        val_loss1 = 0.0\n",
    "        val_loss2 = 0.0\n",
    "        val_loss3 = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Val_Loader\"):\n",
    "\n",
    "                (val_pressureX, val_pressureY, \n",
    "                 val_flowX, val_flowY,\n",
    "                 val_demandX, val_demandY)= data\n",
    "                val_pressureX = val_pressureX.squeeze(0)\n",
    "                val_flowX     = val_flowX.squeeze(0)\n",
    "                val_demandX   = val_demandX.squeeze(0)\n",
    "\n",
    "                # Random Masking\n",
    "                (mask1_pressure, mask1_flow, mask1_demand)=apply_mask(args, val_pressureX, val_flowX, val_demandX)\n",
    "                val_pressureX_ = val_pressureX * mask1_pressure\n",
    "                val_flowX_     = val_flowX * mask1_flow\n",
    "                val_demandX_   = (val_demandX[:, :-1]) * mask1_demand\n",
    "\n",
    "                pressure_Y, flow_Y, demand_Y = model(Graph_Data, mask1_pressure, mask1_flow, mask1_demand, val_pressureX_, val_flowX_, val_demandX_)\n",
    "                # Add last column to demand_Y\n",
    "                row_sums = torch.sum(demand_Y, dim=1, keepdim=True)\n",
    "                demand_Y = torch.cat((demand_Y, -row_sums), dim=1)\n",
    "                \n",
    "                # min-max normalized reduction\n",
    "                pressure_Y = (pressure_Y * (max_pressure - min_pressure) + min_pressure)\n",
    "                flow_Y     = (flow_Y     * (max_flow     - min_flow)     + min_flow)\n",
    "                demand_Y   = (demand_Y   * (max_demand   - min_demand)   + min_demand)\n",
    "                val_pressureX  = (val_pressureX * (max_pressure - min_pressure) + min_pressure)\n",
    "                val_flowX      = (val_flowX     * (max_flow     - min_flow)     + min_flow)\n",
    "                val_demandX    = (val_demandX   * (max_demand   - min_demand)   + min_demand)\n",
    "                val_pressureX_ = (val_pressureX_ * (max_pressure- min_pressure) + min_pressure)\n",
    "                val_flowX_     = (val_flowX_     * (max_flow    - min_flow)     + min_flow)\n",
    "                val_demandX_   = (val_demandX_   * (max_demand   - min_demand)  + min_demand)\n",
    "\n",
    "                # Caculate Loss\n",
    "                (loss1, loss2, loss3) = criterion(pressure_Y, flow_Y, demand_Y, pipe_friction, incidence_matrix, cycle_matrix)\n",
    "                \n",
    "                # Save\n",
    "                pressure_True = torch.cat((pressure_True, val_pressureX), 0)\n",
    "                pressure_Est  = torch.cat((pressure_Est, pressure_Y), 0)\n",
    "                flow_True     = torch.cat((flow_True,   val_flowX), 0)\n",
    "                flow_Est      = torch.cat((flow_Est,     flow_Y), 0)\n",
    "                demand_True   = torch.cat((demand_True, val_demandX), 0)\n",
    "                demand_Est    = torch.cat((demand_Est,   demand_Y), 0)\n",
    "\n",
    "                # Loss\n",
    "                val_loss1 += loss1.item()\n",
    "                val_loss2 += loss2.item()\n",
    "                val_loss3 += loss3.item()\n",
    "        avg_val_loss1 = (val_loss1) / len(val_loader.dataset)\n",
    "        avg_val_loss2 = (val_loss2) / len(val_loader.dataset)\n",
    "        avg_val_loss3 = (val_loss3) / len(val_loader.dataset)\n",
    "        avg_val_loss = (avg_val_loss1 + avg_val_loss2 + avg_val_loss3)\n",
    "\n",
    "        val_loss1_list.append(avg_val_loss1)\n",
    "        val_loss2_list.append(avg_val_loss2)\n",
    "        val_loss3_list.append(avg_val_loss3)\n",
    "        \n",
    "        IO.cprint('Epoch #{:03d}, Conservation1: {:.4f}, Conservation2: {:.4f}, Conservation3: {:.4f}'.format(\n",
    "                epoch, (avg_val_loss1), (avg_val_loss2), (avg_val_loss3)\n",
    "                ))\n",
    "\n",
    "        # Choose the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save the best model\n",
    "            best_model_wts = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Check for early stopping\n",
    "        if patience_counter >= patience:\n",
    "            IO.cprint('Early stopping triggered. Best Val_Loss: {:.4f}'.format(best_val_loss))\n",
    "            break\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    torch.save(model, 'outputs/%s/model.pth' % args.exp_name)\n",
    "    IO.cprint('The current best model is saved in: {}'.format('******** outputs/%s/model.pth *********' % args.exp_name))\n",
    "    end_time = time.time()\n",
    "    IO.cprint('Total time: {:.4f}s'.format(end_time - start_time))\n",
    "    return (pressure_True, pressure_Est, flow_True, flow_Est, demand_True, demand_Est,\n",
    "            train_loss1_list, train_loss2_list, train_loss3_list, \n",
    "            val_loss1_list, val_loss2_list, val_loss3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, IO, test_loader, \n",
    "        min_pressure, max_pressure, min_flow, max_flow, min_demand, max_demand):\n",
    "    \"\"\"测试模型\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    Graph_Data, pipe_friction, incidence_matrix, cycle_matrix = load_matrix(args)\n",
    "    \n",
    "    IO.cprint('')\n",
    "    IO.cprint('********** TEST START **********')\n",
    "    IO.cprint('Reload Best Model')\n",
    "    IO.cprint('The current best model is saved in: {}'.format('******** outputs/%s/model.pth *********' % args.exp_name))\n",
    "\n",
    "    model = torch.load('outputs/%s/model.pth' % args.exp_name).to(device)\n",
    "    model = model.train()\n",
    "    \n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=1e-7)\n",
    "                                        \n",
    "    ##############################\n",
    "    ### Test For Generalization ##\n",
    "    ##############################\n",
    "\n",
    "    # Loss Function\n",
    "    criterion1 = ConservationConstraints()\n",
    "    criterion2 = ConservationConstraints_generalization()\n",
    "\n",
    "    offset = 0  \t\t# initial offset\n",
    "    batch_size = 3 * 96 # from 3-days ago\n",
    "\n",
    "    MAE_pressure = []\n",
    "    MAE_flow = []\n",
    "    MAE_demand = []\n",
    "    R2_pressure = []\n",
    "    R2_flow = []\n",
    "    R2_demand = []\n",
    "    Loss1 = []\n",
    "    Loss2 = []\n",
    "    Loss3 = []\n",
    "\n",
    "    for loop_ in range(len(test_loader) - 3 * 96):\n",
    "\n",
    "        start_index = offset\n",
    "        end_index = min(start_index + batch_size, len(test_loader))\n",
    "        sliced_dataset = torch.utils.data.Subset(test_loader.dataset, range(start_index, end_index))\n",
    "        new_test_loader = torch.utils.data.DataLoader(sliced_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        for i, data in tqdm(enumerate(new_test_loader), total=len(new_test_loader), desc=\"Test_Loader\"):\n",
    "            (test_pressureX, test_pressureY,\n",
    "            test_flowX, test_flowY, \n",
    "            test_demandX, test_demandY)= data\n",
    "            test_pressureX = test_pressureX.squeeze(0)\n",
    "            test_flowX     = test_flowX.squeeze(0)\n",
    "            test_demandX   = test_demandX.squeeze(0)\n",
    "\n",
    "            (mask1_pressure, mask1_flow, mask1_demand)=apply_mask(args, test_pressureX, test_flowX, test_demandX)\n",
    "\n",
    "            test_pressureX_ = test_pressureX * mask1_pressure\n",
    "            test_flowX_     = test_flowX * mask1_flow\n",
    "            test_demandX_   = (test_demandX[:, :-1]) * mask1_demand\n",
    "\n",
    "            pressure_Y, flow_Y, demand_Y = model(Graph_Data, mask1_pressure, mask1_flow, mask1_demand, test_pressureX_, test_flowX_ , test_demandX_)\n",
    "            row_sums = torch.sum(demand_Y, dim=1, keepdim=True)\n",
    "            demand_Y = torch.cat((demand_Y, -row_sums), dim=1)\n",
    "        \n",
    "            pressure_Y = (pressure_Y * (max_pressure - min_pressure) + min_pressure)\n",
    "            flow_Y     = (flow_Y     * (max_flow     - min_flow)     + min_flow)\n",
    "            demand_Y   = (demand_Y   * (max_demand   - min_demand)   + min_demand)\n",
    "            test_pressureX = (test_pressureX * (max_pressure - min_pressure) + min_pressure)\n",
    "            test_flowX     = (test_flowX     * (max_flow     - min_flow)     + min_flow)\n",
    "            test_demandX   = (test_demandX   * (max_demand   - min_demand)   + min_demand)\n",
    "\n",
    "            (loss1, loss2, loss3) = criterion1(pressure_Y, flow_Y, demand_Y, pipe_friction, incidence_matrix, cycle_matrix)\n",
    "            loss_ = loss1 + loss2 + loss3\n",
    "            loss_.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Save the last batch\n",
    "            if i == len(new_test_loader) - 1:\n",
    "\n",
    "                IO.cprint('Loss1: {:.4f}, Loss2: {:.4f}, Loss3: {:.4f}'.format(loss1.item(), loss2.item(), loss3.item()))\n",
    "                (loss1, loss2, loss3) = criterion2(pressure_Y, flow_Y, demand_Y, pipe_friction, incidence_matrix, cycle_matrix)\n",
    "\n",
    "                mae_pressure = torch.mean(torch.abs(pressure_Y - test_pressureX), dim=0)\n",
    "                mae_flow     = torch.mean(torch.abs(flow_Y - test_flowX), dim=0)\n",
    "                mae_demand   = torch.mean(torch.abs(demand_Y - test_demandX), dim=0)\n",
    "\n",
    "                r2_pressure = 1 - (torch.sum(((pressure_Y - test_pressureX) ** 2), dim=0) / torch.sum((pressure_Y - torch.mean(pressure_Y)) ** 2, dim=0))\n",
    "                r2_flow     = 1 - (torch.sum(((flow_Y - test_flowX) ** 2), dim=0) / torch.sum((flow_Y - torch.mean(flow_Y)) ** 2, dim=0))\n",
    "                r2_demand   = 1 - (torch.sum(((demand_Y - test_demandX) ** 2), dim=0) / torch.sum((demand_Y - torch.mean(demand_Y)) ** 2, dim=0))\n",
    "\n",
    "                Loss1.append(loss1.detach().cpu().numpy())\n",
    "                Loss2.append(loss2.detach().cpu().numpy())\n",
    "                Loss3.append(loss3.detach().cpu().numpy())\n",
    "\n",
    "                MAE_pressure.append(mae_pressure.detach().cpu().numpy())\n",
    "                MAE_flow.append(mae_flow.detach().cpu().numpy())\n",
    "                MAE_demand.append(mae_demand.detach().cpu().numpy())\n",
    "\n",
    "                R2_pressure.append(r2_pressure.detach().cpu().numpy())\n",
    "                R2_flow.append(r2_flow.detach().cpu().numpy())  \n",
    "                R2_demand.append(r2_demand.detach().cpu().numpy())\n",
    "\n",
    "                offset += 1\n",
    "\n",
    "            del pressure_Y, flow_Y, demand_Y, loss1, loss2, loss3\n",
    "            del test_pressureX, test_flowX, test_demandX, test_pressureX_, test_flowX_, test_demandX_\n",
    "\n",
    "    return (Loss1, Loss2, Loss3,\n",
    "            MAE_pressure, MAE_flow, MAE_demand,\n",
    "            R2_pressure, R2_flow, R2_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "def exp_init():\n",
    "    if not os.path.exists('outputs'):\n",
    "        os.mkdir('outputs')\n",
    "    if not os.path.exists('outputs/' + args.exp_name):\n",
    "        os.mkdir('outputs/' + args.exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------------------------------------------+\n",
      "|      Parameter       |                         Value                         |\n",
      "+======================+=======================================================+\n",
      "| Chebyshevdegree      | 8                                                     |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Fourierdegree        | 6                                                     |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Batch size           | 96                                                    |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Cycle file           | D:/GraphormerForRobustness/dataset2/initial_cycle_mat |\n",
      "|                      | rix_dir.csv                                           |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Decay epoch          | 1                                                     |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Demand sensor name   | [\"2\", \"16\", \"19\", \"22\", \"24\"]                         |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Device               | cuda                                                  |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Edge adjacency file  | D:/GraphormerForRobustness/dataset/Apulia/edge_adjace |\n",
      "|                      | ncy_matrix.csv                                        |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Edge file            | D:/GraphormerForRobustness/dataset2/edge_index_dir.cs |\n",
      "|                      | v                                                     |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Exp name             | Exp                                                   |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Flow sensor name     | [\"4\", \"7\", \"25\", \"28\", \"34\"]                          |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Gamma                | 0.95                                                  |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Hidden dim           | 12                                                    |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Incidence file       | D:/GraphormerForRobustness/dataset2/initial_incidence |\n",
      "|                      | _matrix_dir.csv                                       |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| L1 lambda            | 0.0                                                   |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| L2 lambda            | 0.0                                                   |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Learning rate        | 2e-05                                                 |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Max epoch            | 50                                                    |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Node adjacency file  | D:/GraphormerForRobustness/dataset/Apulia/node_adjace |\n",
      "|                      | ncy_matrix.csv                                        |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Node file            | D:/GraphormerForRobustness/dataset/Apulia_2/node_feat |\n",
      "|                      | ures_Apulia2.csv                                      |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Num edges            | 34                                                    |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Num history          | 1                                                     |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Num nodes            | 24                                                    |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Num step             | 8640                                                  |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Patience             | 10                                                    |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Pressure sensor name | [\"4\", \"13\", \"14\", \"23\", \"24\"]                         |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Seed                 | 10                                                    |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Test ratio           | 0.3333333333333333                                    |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Train ratio          | 0.5                                                   |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Val ratio            | 0.16666666666666666                                   |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Water demand file    | D:/GraphormerForRobustness/dataset2/demand_patten_361 |\n",
      "|                      | 2/simulate_demand3.csv                                |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Water flow file      | D:/GraphormerForRobustness/dataset2/demand_patten_361 |\n",
      "|                      | 2/simulate_flow3.csv                                  |\n",
      "+----------------------+-------------------------------------------------------+\n",
      "| Water pressure file  | D:/GraphormerForRobustness/dataset2/demand_patten_361 |\n",
      "|                      | 2/simulate_pressure3.csv                              |\n",
      "+----------------------+-------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AppData\\Local\\Temp\\ipykernel_56716\\2426456413.py:14: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(self.fouriercoeffs, mean=0.0, std=1 / (np.sqrt(input_dim) * np.sqrt(self.gridsize)))\n",
      "C:\\AppData\\Local\\Temp\\ipykernel_56716\\1574067456.py:10: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(self.cheby_coeffs, mean=0.0, std=1/(input_dim * (degree + 1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KANSA(\n",
      "  (flow_batch): Sequential(\n",
      "    (0): Linear(in_features=96, out_features=12, bias=True)\n",
      "    (1): Linear(in_features=12, out_features=96, bias=True)\n",
      "    (2): FourierKANLayer()\n",
      "    (3): FourierKANLayer()\n",
      "    (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pressure_batch): Sequential(\n",
      "    (0): Linear(in_features=96, out_features=12, bias=True)\n",
      "    (1): Linear(in_features=12, out_features=96, bias=True)\n",
      "    (2): FourierKANLayer()\n",
      "    (3): FourierKANLayer()\n",
      "    (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (demand_batch): Sequential(\n",
      "    (0): Linear(in_features=96, out_features=12, bias=True)\n",
      "    (1): Linear(in_features=12, out_features=96, bias=True)\n",
      "    (2): FourierKANLayer()\n",
      "    (3): FourierKANLayer()\n",
      "    (4): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (flow_features): Sequential(\n",
      "    (0): Linear(in_features=34, out_features=12, bias=True)\n",
      "    (1): Linear(in_features=12, out_features=34, bias=True)\n",
      "    (2): ChebyshevKANLayer()\n",
      "    (3): ChebyshevKANLayer()\n",
      "    (4): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pressure_features): Sequential(\n",
      "    (0): Linear(in_features=24, out_features=12, bias=True)\n",
      "    (1): Linear(in_features=12, out_features=24, bias=True)\n",
      "    (2): ChebyshevKANLayer()\n",
      "    (3): ChebyshevKANLayer()\n",
      "    (4): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (demand_features): Sequential(\n",
      "    (0): Linear(in_features=23, out_features=12, bias=True)\n",
      "    (1): Linear(in_features=12, out_features=23, bias=True)\n",
      "    (2): ChebyshevKANLayer()\n",
      "    (3): ChebyshevKANLayer()\n",
      "    (4): LayerNorm((23,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (attention_pressure_batch): Attention(\n",
      "    (query_layer): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (key_layer): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (value_layer): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (layer_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (attention_flow_batch): Attention(\n",
      "    (query_layer): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (key_layer): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (value_layer): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (layer_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (attention_demand_batch): Attention(\n",
      "    (query_layer): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (key_layer): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (value_layer): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (layer_norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (attention_pressure_features): Attention(\n",
      "    (query_layer): Linear(in_features=24, out_features=24, bias=True)\n",
      "    (key_layer): Linear(in_features=24, out_features=24, bias=True)\n",
      "    (value_layer): Linear(in_features=24, out_features=24, bias=True)\n",
      "    (layer_norm): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (attention_flow_features): Attention(\n",
      "    (query_layer): Linear(in_features=34, out_features=34, bias=True)\n",
      "    (key_layer): Linear(in_features=34, out_features=34, bias=True)\n",
      "    (value_layer): Linear(in_features=34, out_features=34, bias=True)\n",
      "    (layer_norm): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (attention_demand_features): Attention(\n",
      "    (query_layer): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (key_layer): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (value_layer): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (layer_norm): LayerNorm((23,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pressure_1): Sequential(\n",
      "    (0): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (pressure_2): Sequential(\n",
      "    (0): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (flow_1): Sequential(\n",
      "    (0): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (flow_2): Sequential(\n",
      "    (0): LayerNorm((34,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (demand_1): Sequential(\n",
      "    (0): LayerNorm((23,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (demand_2): Sequential(\n",
      "    (0): LayerNorm((23,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (pressure_sum): Sequential(\n",
      "    (0): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (flow_sum): Sequential(\n",
      "    (0): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (demand_sum): Sequential(\n",
      "    (0): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (graph): Sequential(\n",
      "    (0): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): ChebyshevKANLayer()\n",
      "  )\n",
      "  (flow_out): Sequential(\n",
      "    (0): ChebyshevKANLayer()\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): Linear(in_features=34, out_features=34, bias=True)\n",
      "    (3): Linear(in_features=34, out_features=34, bias=True)\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      "  (pressure_out): Sequential(\n",
      "    (0): ChebyshevKANLayer()\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): Linear(in_features=24, out_features=24, bias=True)\n",
      "    (3): Linear(in_features=24, out_features=24, bias=True)\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      "  (demand_out): Sequential(\n",
      "    (0): ChebyshevKANLayer()\n",
      "    (1): ChebyshevKANLayer()\n",
      "    (2): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (3): Linear(in_features=23, out_features=23, bias=True)\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Model Parameter: 333688\n",
      "Using RMSprop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [09:30<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #000, Conservation1: 467070.0138, Conservation2: 21254955.9058, Conservation3: 26147178.2699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [00:53<00:00, 23.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #000, Conservation1: 1010.5197, Conservation2: 106.9387, Conservation3: 1163.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [10:18<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #001, Conservation1: 687.0553, Conservation2: 76.5890, Conservation3: 927.4044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:11<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #001, Conservation1: 246.6851, Conservation2: 33.5862, Conservation3: 361.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [10:35<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #002, Conservation1: 186.2768, Conservation2: 29.7768, Conservation3: 279.1882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:16<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #002, Conservation1: 103.8278, Conservation2: 29.3487, Conservation3: 176.9509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [10:27<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #003, Conservation1: 105.5992, Conservation2: 21.3788, Conservation3: 152.9279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:19<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #003, Conservation1: 88.3545, Conservation2: 33.7687, Conservation3: 190.7471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [10:52<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #004, Conservation1: 75.1331, Conservation2: 17.6548, Conservation3: 109.0948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:22<00:00, 15.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #004, Conservation1: 67.0370, Conservation2: 24.5351, Conservation3: 129.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:01<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #005, Conservation1: 52.5229, Conservation2: 14.9855, Conservation3: 83.4094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:25<00:00, 14.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #005, Conservation1: 39.6352, Conservation2: 19.3222, Conservation3: 87.7997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:20<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #006, Conservation1: 38.1092, Conservation2: 12.3963, Conservation3: 65.5024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:26<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #006, Conservation1: 34.4009, Conservation2: 10.4323, Conservation3: 82.8742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:20<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #007, Conservation1: 29.9445, Conservation2: 10.4762, Conservation3: 52.7569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:27<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #007, Conservation1: 23.2165, Conservation2: 14.3018, Conservation3: 55.7260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:27<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #008, Conservation1: 24.6463, Conservation2: 9.1361, Conservation3: 43.6693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:29<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #008, Conservation1: 26.8253, Conservation2: 7.9837, Conservation3: 61.0921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:20<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #009, Conservation1: 20.7337, Conservation2: 8.1387, Conservation3: 37.1652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:30<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #009, Conservation1: 14.4570, Conservation2: 8.2727, Conservation3: 31.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:28<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #010, Conservation1: 17.5925, Conservation2: 7.2959, Conservation3: 32.1016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:32<00:00, 13.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #010, Conservation1: 12.0689, Conservation2: 8.1481, Conservation3: 29.0095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:37<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #011, Conservation1: 15.1556, Conservation2: 6.5509, Conservation3: 27.9540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:36<00:00, 12.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #011, Conservation1: 20.2402, Conservation2: 25.2246, Conservation3: 69.1549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:27<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #012, Conservation1: 13.1669, Conservation2: 5.9591, Conservation3: 24.6002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:39<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #012, Conservation1: 11.5917, Conservation2: 8.0640, Conservation3: 35.9489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:32<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #013, Conservation1: 11.5968, Conservation2: 5.3600, Conservation3: 21.7516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:35<00:00, 13.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #013, Conservation1: 11.1703, Conservation2: 3.3520, Conservation3: 28.5669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:30<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #014, Conservation1: 10.2217, Conservation2: 4.8976, Conservation3: 19.3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:38<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #014, Conservation1: 10.7397, Conservation2: 3.2959, Conservation3: 28.5758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:40<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #015, Conservation1: 9.1658, Conservation2: 4.4647, Conservation3: 17.3035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:39<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #015, Conservation1: 8.9053, Conservation2: 2.5052, Conservation3: 22.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:34<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #016, Conservation1: 8.1914, Conservation2: 4.0885, Conservation3: 15.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:39<00:00, 12.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #016, Conservation1: 9.1846, Conservation2: 2.9467, Conservation3: 21.7363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:44<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #017, Conservation1: 7.4068, Conservation2: 3.7464, Conservation3: 14.0904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:41<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #017, Conservation1: 7.9357, Conservation2: 2.3411, Conservation3: 22.6247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:51<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #018, Conservation1: 6.7235, Conservation2: 3.4487, Conservation3: 12.7919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:41<00:00, 12.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #018, Conservation1: 6.0711, Conservation2: 4.5482, Conservation3: 18.8075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:23<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #019, Conservation1: 6.1273, Conservation2: 3.1757, Conservation3: 11.6746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:33<00:00, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #019, Conservation1: 6.7144, Conservation2: 2.1694, Conservation3: 20.4767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:01<00:00,  5.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #020, Conservation1: 5.5977, Conservation2: 2.9332, Conservation3: 10.6799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:46<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #020, Conservation1: 5.2046, Conservation2: 3.6102, Conservation3: 14.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:03<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #021, Conservation1: 5.1211, Conservation2: 2.7150, Conservation3: 9.8339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:46<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #021, Conservation1: 4.5113, Conservation2: 2.5998, Conservation3: 13.3247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:00<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #022, Conservation1: 4.6994, Conservation2: 2.5126, Conservation3: 9.0403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:45<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #022, Conservation1: 5.4636, Conservation2: 1.8151, Conservation3: 16.6655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:00<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #023, Conservation1: 4.3470, Conservation2: 2.3387, Conservation3: 8.3522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:52<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #023, Conservation1: 5.5478, Conservation2: 2.0895, Conservation3: 16.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:06<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #024, Conservation1: 4.0214, Conservation2: 2.1788, Conservation3: 7.7355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:49<00:00, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #024, Conservation1: 4.9514, Conservation2: 1.8129, Conservation3: 14.0920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:15<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #025, Conservation1: 3.7057, Conservation2: 2.0260, Conservation3: 7.1485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:50<00:00, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #025, Conservation1: 3.1344, Conservation2: 1.2158, Conservation3: 6.6725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:55<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #026, Conservation1: 3.4354, Conservation2: 1.8884, Conservation3: 6.6524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:45<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #026, Conservation1: 3.1923, Conservation2: 3.0224, Conservation3: 10.5638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [11:58<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #027, Conservation1: 3.2123, Conservation2: 1.7678, Conservation3: 6.1874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:44<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #027, Conservation1: 3.3467, Conservation2: 2.8953, Conservation3: 8.7506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:27<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #028, Conservation1: 2.9918, Conservation2: 1.6430, Conservation3: 5.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:51<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #028, Conservation1: 3.6497, Conservation2: 1.2954, Conservation3: 10.3807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:20<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #029, Conservation1: 2.7920, Conservation2: 1.5426, Conservation3: 5.3939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:52<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #029, Conservation1: 2.6947, Conservation2: 1.3222, Conservation3: 7.1633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:16<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #030, Conservation1: 2.6131, Conservation2: 1.4435, Conservation3: 5.0478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:50<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #030, Conservation1: 3.0140, Conservation2: 1.9697, Conservation3: 6.9055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:49<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #031, Conservation1: 2.4541, Conservation2: 1.3508, Conservation3: 4.7430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:00<00:00, 10.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #031, Conservation1: 3.0627, Conservation2: 1.2191, Conservation3: 7.4430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:28<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #032, Conservation1: 2.3230, Conservation2: 1.2692, Conservation3: 4.4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [01:54<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #032, Conservation1: 3.1592, Conservation2: 1.1957, Conservation3: 8.8061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [12:42<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #033, Conservation1: 2.1863, Conservation2: 1.1939, Conservation3: 4.1966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:00<00:00, 10.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #033, Conservation1: 2.3767, Conservation2: 1.9048, Conservation3: 8.3630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [13:41<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #034, Conservation1: 2.0882, Conservation2: 1.1212, Conservation3: 3.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:08<00:00,  9.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #034, Conservation1: 2.4225, Conservation2: 0.9640, Conservation3: 6.5826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [13:18<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #035, Conservation1: 1.9784, Conservation2: 1.0584, Conservation3: 3.7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:06<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #035, Conservation1: 2.4547, Conservation2: 1.4049, Conservation3: 6.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [13:22<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #036, Conservation1: 1.8811, Conservation2: 0.9978, Conservation3: 3.5478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:10<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #036, Conservation1: 2.4880, Conservation2: 1.0737, Conservation3: 6.8862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [13:37<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #037, Conservation1: 1.8072, Conservation2: 0.9433, Conservation3: 3.3762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:10<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #037, Conservation1: 2.5185, Conservation2: 1.1325, Conservation3: 6.9087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [13:42<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #038, Conservation1: 1.7225, Conservation2: 0.8951, Conservation3: 3.2076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:13<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #038, Conservation1: 2.1084, Conservation2: 1.1501, Conservation3: 5.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [13:36<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #039, Conservation1: 1.6555, Conservation2: 0.8486, Conservation3: 3.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:11<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #039, Conservation1: 2.1756, Conservation2: 0.8974, Conservation3: 6.1581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [14:05<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #040, Conservation1: 1.5885, Conservation2: 0.8070, Conservation3: 2.9132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:16<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #040, Conservation1: 1.7892, Conservation2: 1.0592, Conservation3: 4.1988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [13:54<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #041, Conservation1: 1.5272, Conservation2: 0.7671, Conservation3: 2.7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:16<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #041, Conservation1: 1.6086, Conservation2: 0.9141, Conservation3: 4.2117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [14:37<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #042, Conservation1: 1.4737, Conservation2: 0.7321, Conservation3: 2.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:25<00:00,  8.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #042, Conservation1: 1.6590, Conservation2: 0.9568, Conservation3: 3.9149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [14:15<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #043, Conservation1: 1.4098, Conservation2: 0.6984, Conservation3: 2.5535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:22<00:00,  8.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #043, Conservation1: 1.7314, Conservation2: 0.9075, Conservation3: 4.3346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [14:10<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #044, Conservation1: 1.3638, Conservation2: 0.6688, Conservation3: 2.4516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:20<00:00,  8.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #044, Conservation1: 1.6349, Conservation2: 0.8846, Conservation3: 4.3793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [14:07<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #045, Conservation1: 1.3180, Conservation2: 0.6410, Conservation3: 2.3573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:20<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #045, Conservation1: 1.4376, Conservation2: 0.9905, Conservation3: 3.8973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [14:29<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #046, Conservation1: 1.2832, Conservation2: 0.6141, Conservation3: 2.2726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:23<00:00,  8.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #046, Conservation1: 1.5107, Conservation2: 0.9865, Conservation3: 4.6904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [14:03<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #047, Conservation1: 1.2418, Conservation2: 0.5899, Conservation3: 2.1913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:18<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #047, Conservation1: 1.4718, Conservation2: 0.8118, Conservation3: 3.6292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [14:36<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #048, Conservation1: 1.2041, Conservation2: 0.5677, Conservation3: 2.1149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader: 100%|██████████| 1249/1249 [02:23<00:00,  8.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #048, Conservation1: 1.4989, Conservation2: 0.6284, Conservation3: 3.8995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_Loader: 100%|██████████| 4129/4129 [14:44<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #049, Conservation1: 1.1759, Conservation2: 0.5475, Conservation3: 2.0475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val_Loader:   0%|          | 0/1249 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "      random.seed(args.seed) \n",
    "      torch.manual_seed(args.seed) \n",
    "      exp_init()      \n",
    "      IO = IOStream('outputs/' + args.exp_name + '/run.log')\n",
    "      IO.cprint(str(table_printer(args)))  \n",
    "\n",
    "      train_dataset = CustomDataset(args, dataset_type='train')\n",
    "      val_dataset = CustomDataset(args, dataset_type='val')\n",
    "      test_dataset = CustomDataset(args, dataset_type='test')\n",
    "      train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "      val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "      test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "\n",
    "      NodePressure = pd.read_csv(args.water_pressure_file, header=0, index_col=0)\n",
    "      PipeFlow = pd.read_csv(args.water_flow_file, header=0, index_col=0)\n",
    "      NodeDemand = pd.read_csv(args.water_demand_file, header=0, index_col=0)\n",
    "      max_pressure = NodePressure.max().max()\n",
    "      min_pressure = NodePressure.min().min()\n",
    "      max_flow = PipeFlow.max().max() \n",
    "      min_flow = PipeFlow.min().min()\n",
    "      max_demand = NodeDemand.max().max()\n",
    "      min_demand = (NodeDemand.iloc[:, :-1]).min().min()                          \n",
    "\n",
    "      min_pressure = torch.tensor(min_pressure).to(args.device)\n",
    "      max_pressure = torch.tensor(max_pressure).to(args.device)\n",
    "      min_flow = torch.tensor(min_flow).to(args.device)\n",
    "      max_flow = torch.tensor(max_flow).to(args.device)\n",
    "\n",
    "\t  # TODO ： If you are just testing the model, just mask the following line of code to run the model\n",
    "      (pressure_True, pressure_Est, flow_True, flow_Est, demand_True, demand_Est,\n",
    "       train_loss1_list, train_loss2_list, train_loss3_list,\n",
    "       val_loss1_list, val_loss2_list, val_loss3_list) = train(\n",
    "       args, IO, train_dataloader, val_dataloader, min_pressure, max_pressure, min_flow, max_flow, min_demand, max_demand)\n",
    "      \n",
    "      (Loss1, Loss2, Loss3,\n",
    "       MAE_pressure, MAE_flow, MAE_demand,\n",
    "       R2_pressure, R2_flow, R2_demand) = test(\n",
    "      args, IO, test_dataloader, min_pressure, max_pressure, min_flow, max_flow, min_demand, max_demand) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'outputs/DataManage' \n",
    "os.makedirs(output_dir, exist_ok=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 数据输出到csv文件 \n",
    "# save_pressure_True = pressure_True.cpu().detach().numpy().astype(float)\n",
    "# save_pressure_Est = pressure_Est.cpu().detach().numpy().astype(float)\n",
    "# save_flow_True = flow_True.cpu().detach().numpy().astype(float)\n",
    "# save_flow_Est = flow_Est.cpu().detach().numpy().astype(float)\n",
    "# save_demand_True = demand_True.cpu().detach().numpy().astype(float)\n",
    "# save_demand_Est = demand_Est.cpu().detach().numpy().astype(float)\n",
    "\n",
    "# np.savetxt(os.path.join(output_dir, 'pressure_True.csv'), save_pressure_True, delimiter=',')\n",
    "# np.savetxt(os.path.join(output_dir, 'pressure_Est.csv'), save_pressure_Est, delimiter=',')\n",
    "# np.savetxt(os.path.join(output_dir, 'flow_True.csv'), save_flow_True, delimiter=',')  \n",
    "# np.savetxt(os.path.join(output_dir, 'flow_Est.csv'), save_flow_Est, delimiter=',')\n",
    "# np.savetxt(os.path.join(output_dir, 'demand_True.csv'), save_demand_True, delimiter=',')\n",
    "# np.savetxt(os.path.join(output_dir, 'demand_Est.csv'), save_demand_Est, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generalization (MAE_pressure, MAE_flow, MAE_demand, R2_pressure, R2_flow, R2_demand) \n",
    "MAE_pressure = pd.DataFrame(MAE_pressure)\n",
    "MAE_flow = pd.DataFrame(MAE_flow)\n",
    "MAE_demand = pd.DataFrame(MAE_demand)\n",
    "\n",
    "R2_pressure = pd.DataFrame(R2_pressure)\n",
    "R2_flow = pd.DataFrame(R2_flow)\n",
    "R2_demand = pd.DataFrame(R2_demand)\n",
    "\n",
    "Loss1 = pd.DataFrame(Loss1)\n",
    "Loss2 = pd.DataFrame(Loss2)\n",
    "Loss3 = pd.DataFrame(Loss3)\n",
    "Loss1.to_csv(os.path.join(output_dir, 'Coservation1_generalization.csv'), index=False)\n",
    "Loss2.to_csv(os.path.join(output_dir, 'Coservation2_generalization.csv'), index=False)\n",
    "Loss3.to_csv(os.path.join(output_dir, 'Coservation3_generalization.csv'), index=False)\n",
    "\n",
    "MAE_pressure.to_csv(os.path.join(output_dir, 'MAE_pressure_generalization.csv'), index=False)\n",
    "MAE_flow.to_csv(os.path.join(output_dir, 'MAE_flow_generalization.csv'), index=False)\n",
    "MAE_demand.to_csv(os.path.join(output_dir, 'MAE_demand_generalization.csv'), index=False)\n",
    "\n",
    "R2_pressure.to_csv(os.path.join(output_dir, 'R2_pressure_generalization.csv'), index=False)\n",
    "R2_flow.to_csv(os.path.join(output_dir, 'R2_flow_generalization.csv'), index=False)\n",
    "R2_demand.to_csv(os.path.join(output_dir, 'R2_demand_generalization.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
